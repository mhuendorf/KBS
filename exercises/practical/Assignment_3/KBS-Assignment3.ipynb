{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8debce11",
   "metadata": {},
   "source": [
    "## Exercise 03.01: Scikit Learn, Interpretable Machine Learning\n",
    "\n",
    "\n",
    "Part 1: Learn about Scikit Learn (Machine Learning in Python, e.g.\n",
    "* https://scikit-learn.org/stable/tutorial/index.html\n",
    "* Video: http://videolectures.net/icml2010_varaquaux_scik/\n",
    "* Another alternative (*): https://www.youtube.com/watch?v=0Lt9w-BxKFQ\n",
    "\n",
    "Part 2: Learn about feature importance and SHAP\n",
    "* https://towardsdatascience.com/interpretable-machine-learning-models-aef0c7be3fd9\n",
    "* https://shap.readthedocs.io/en/latest/index.html\n",
    "* Video: https://www.youtube.com/watch?v=GzEgxFtHH4w\n",
    "\n",
    "Part 3: Apply Scikit Learn & SHAP\n",
    "* Use the heart disease dataset: https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "* Apply preprocessing on the data as needed, see the (*) video, also provide some overview/visualization of the features \n",
    "* Build models using logistic regression, decision trees (C4.5), random forest, neural networks and evaluate them (accuracy, ROC/AUC)\n",
    "* For the evaluation, you can print some tables or use plots\n",
    "* Which are the important features of the model(s)? Use feature importance for the tree-based models. Are there any differences there?\n",
    "* Apply SHAP on all the models, and provide a view on the important features and their impact\n",
    "* Discuss (write some text!) about which model works best, and which is \"most interpretable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dca1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ... add here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb203856",
   "metadata": {},
   "source": [
    "## Exercise 03.02: Reading/Discussion/Summary\n",
    "\n",
    "Part 1: Reading:\n",
    "* Read the following paper: Rudin (2019) \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\"\n",
    "* The paper is available here: https://www.nature.com/articles/s42256-019-0048-x\n",
    "\n",
    "Part 2: Think about the following questions:\n",
    "* What is Explainable Machine Learning?\n",
    "* What are the features of Interpretable Machine Learning?\n",
    "* Why and when do we need both?\n",
    "* Are there any disadvantages?\n",
    "* What are specific challenges in their application?\n",
    "* Can, and if so, how, explainable and interpretable ML be implemented efficiently?\n",
    "* What are some exemplary techniques to apply?\n",
    "\n",
    "Part 3: Discussing, Summary\n",
    "* Prepare answers for these questions for the practical session on November 16, 2021. You will first discuss these in groups, and then we will discuss them in the plenary meeting.\n",
    "* After that, summarize your findings (and those of the group discussion) in a small report (max. half a Din A4 page). For example, you could write 2-3 sentences for answering a specific question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe30961",
   "metadata": {},
   "source": [
    "## Uploading your solution\n",
    "For uploading your solution, please upload two files:\n",
    "* The Jupyter-Notebook file (.ipynb)\n",
    "* A PDF (printout/file) of the Jupyter notebook file (.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda5847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
